# -*- coding: utf-8 -*-
"""Capstone project

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_VQaK9OK0ybXJVxQIcuKgbOt4nEedpvK

***Phase one: Preprocessing the data***

Part a: Data cleaning
"""

import pandas as pd
from google.colab import files
# Uploading the Excel file
uploaded = files.upload()

#data_path = 'test_data.csv'
data_path = 'job_5000.csv'
data = pd.read_csv(data_path)

"""Ali's test code:

"""

#Ali's modification of determining pay period and remote work
# Define the function to determine pay_period based on med_salary
def determine_pay_period(med_salary):
    if med_salary < 100:
        return 'HOURLY'
    elif 100 <= med_salary <= 10000:
        return 'MONTHLY'
    else:
        return 'YEARLY'

# Apply the function to fill pay_period
data['pay_period'] = data['med_salary'].apply(determine_pay_period)

# One-hot encode the pay_period column
pay_period_dummies = pd.get_dummies(data['pay_period'], prefix='pay_period')

# Concatenate the original DataFrame with the new one-hot encoded columns
data_encoded = pd.concat([data, pay_period_dummies], axis=1)

# Optionally, you can drop the original pay_period column if you no longer need it
data_encoded.drop('pay_period', axis=1, inplace=True)

# Fill NaN values in 'remote_allowed' with 0 (assuming remote is not allowed)
data_encoded['remote_allowed'] = data_encoded.apply(lambda row: 1 if 'remote' in row['title'].lower() or 'remote' in row['description'].lower() else 0 if pd.isna(row['remote_allowed']) else row['remote_allowed'], axis=1)

# Check for 'Remote' as well
data_encoded['remote_allowed'] = data_encoded.apply(lambda row: 1 if 'Remote' in row['title'] or 'Remote' in row['description'] else 0 if pd.isna(row['remote_allowed']) else row['remote_allowed'], axis=1)

print(data_encoded.isna().sum())
# Display the head of the updated DataFrame
data_encoded.head()

"""Original Code:"""

#ghadi's version of determining pay period and remote work
# Define the function to determine pay_period based on med_salary
def determine_pay_period(med_salary):
    if med_salary < 100:
        return 'HOURLY'
    elif 100 <= med_salary <= 10000:
        return 'MONTHLY'
    else:
        return 'YEARLY'

# Apply the function to fill pay_period
data['pay_period'] = data['med_salary'].apply(determine_pay_period)

# One-hot encode the pay_period column
pay_period_dummies = pd.get_dummies(data['pay_period'], prefix='pay_period')

# Concatenate the original DataFrame with the new one-hot encoded columns
data_encoded = pd.concat([data, pay_period_dummies], axis=1)

# Optionally, you can drop the original pay_period column if you no longer need it
data_encoded.drop('pay_period', axis=1, inplace=True)

# Fill NaN values in 'remote_allowed' with 0 (assuming remote is not allowed)
data_encoded['remote_allowed'].fillna(0, inplace=True)
print(data_encoded.isna().sum())
# Display the head of the updated DataFrame to check the new columns
data_encoded.head()

"""Ali's Test to fill numerics:"""

#Ali version
from sklearn.preprocessing import StandardScaler

# Numeric features
numeric_features = ['max_salary', 'med_salary', 'min_salary']

# Group data by formatted_experience_level and pay_period
grouped_data = data.groupby(['formatted_experience_level', 'pay_period'])

# Impute missing values with median for numeric features within each group
for feature in numeric_features:
    # Calculate median for each group
    group_medians = grouped_data[feature].median()

    # Fill missing values with median of respective group
    for (experience_level, pay_period), group_median in group_medians.items():
        group_indices = (data['formatted_experience_level'] == experience_level) & (data['pay_period'] == pay_period)
        data.loc[group_indices, feature] = data.loc[group_indices, feature].fillna(group_median)

# Fill remaining missing values in numeric features with overall median
data[numeric_features] = data[numeric_features].fillna(data[numeric_features].median())

# Initialize the StandardScaler
scaler = StandardScaler()

# Standardize the numeric features
data[numeric_features] = scaler.fit_transform(data[numeric_features])

data.head()

"""original filling:"""

#ghadi's version
from sklearn.preprocessing import StandardScaler

# Numeric features
numeric_features = ['max_salary', 'med_salary', 'min_salary']

# Impute missing values with median for these numeric features
for feature in numeric_features:
    median_value = data[feature].median()
    data[feature].fillna(median_value, inplace=True)

# Initialize the StandardScaler
scaler = StandardScaler()

# Standardize the numeric features
data_encoded[numeric_features] = scaler.fit_transform(data[numeric_features])

data_encoded.head()

"""original code, everything beyond here is not modified:"""

# Mapping for formatted_work_type to numeric codes
work_type_mapping = {
    'Full-time': 1,
    'Part-time': 2,
    'Contract': 0
}

# Apply the mapping to create a new encoded column
data_encoded['formatted_work_type_encoded'] = data_encoded['formatted_work_type'].map(work_type_mapping)

data_encoded.drop('formatted_work_type', axis=1, inplace=True)

# Display the head of the data to check the new column
data_encoded.head()

# Apply encoding for the currency: 1 for 'USD' and 0 for all others
data_encoded['currency_encoded'] = data_encoded['currency'].map(lambda x: 1 if x == 'USD' else 0)


data_encoded.drop('currency', axis=1, inplace=True)

# Display the head of the DataFrame to check the new column
data_encoded.head()

# Apply encoding for the compensation_type: 0 for 'BASE_SALARY' and 1 for all others
data_encoded['compensation_type_encoded'] = data_encoded['compensation_type'].map(lambda x: 0 if x == 'BASE_SALARY' else 1)

data_encoded.drop('compensation_type', axis=1, inplace=True)

# Display the head of the DataFrame to check the new column
data_encoded.head()

# Drop the 'work_type' column
data_encoded.drop('work_type', axis=1, inplace=True)

# Display the head of the DataFrame to ensure the column has been removed
data_encoded.head()

from sklearn.feature_extraction.text import TfidfVectorizer

# Initialize the TF-IDF Vectorizer
tfidf_vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')

# Fit and transform the skills_desc column
data_encoded['skills_desc'].fillna('', inplace=True)
tfidf_matrix = tfidf_vectorizer.fit_transform(data_encoded['skills_desc'])

# tfidf_matrix can now be used as input for ML models or further analysis

# Add a binary column indicating the presence of skills description
data_encoded['skills_desc_present'] = data_encoded['skills_desc'].apply(lambda x: 0 if pd.isnull(x) or x == '' else 1)

# Display the head of the DataFrame to show the new column
data_encoded.head()

"""# Original title *preprocessing* **bold text**"""

from sklearn.feature_extraction.text import TfidfVectorizer

# Initialize the TF-IDF Vectorizer for the title column
tfidf_vectorizer_title = TfidfVectorizer(max_features=1000, stop_words='english')

# Fit and transform the title column
titles_tfidf_matrix = tfidf_vectorizer_title.fit_transform(data_encoded['title'])

# titles_tfidf_matrix is a sparse matrix containing the TF-IDF vectors for each title
# This matrix can be used as input for various ML models or analysis

"""# ** dr ABBAS Alternative  preprocessing tip**


"""

import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer, WordNetLemmatizer
import string

# Download necessary NLTK resources
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')  # Necessary for lemmatization

# Initializing stemmer and lemmatizer
stemmer = PorterStemmer()
lemmatizer = WordNetLemmatizer()

def preprocess(text):
    # Tokenize and convert to lower case
    tokens = word_tokenize(text.lower())
    # Remove punctuation
    tokens = [word for word in tokens if word not in string.punctuation]
    # Remove stop words
    tokens = [word for word in tokens if word not in stopwords.words('english')]
    # Apply stemming
    tokens = [stemmer.stem(word) for word in tokens]
    # Apply lemmatization
    tokens = [lemmatizer.lemmatize(word) for word in tokens]
    return tokens

# Applying preprocessing to each title
data_encoded['processed_titles'] = data_encoded['title'].apply(preprocess)

# Rest of your code for Word2Vec training and applying vectorization follows...


from gensim.models import Word2Vec

# Training a Word2Vec model
word2vec_model = Word2Vec(sentences=data_encoded['processed_titles'], vector_size=100, window=5, min_count=2, workers=4)

# Save the model for later use
word2vec_model.save("word2vec_title.model")

import numpy as np

# Function to vectorize a title
def vectorize_title(title_tokens, model):
    # Retrieve word vectors for each word in the title
    word_vectors = [model.wv[word] for word in title_tokens if word in model.wv]

    # If there's at least one word with a vector, return the mean vector
    if len(word_vectors) > 0:
        return np.mean(word_vectors, axis=0)
    else:
        # Return a zero vector for titles that have no valid words
        return np.zeros(model.vector_size)

# Apply vectorization to each processed title
data_encoded['title_vector'] = data_encoded['processed_titles'].apply(lambda x: vectorize_title(x, word2vec_model))

# Define an ordinal encoding mapping based on the observed levels in your dataset
experience_level_mapping = {
    'Entry level': 0,
    'Associate': 1,
    'Mid-Senior level': 2,
    'Director': 3,
    'Executive': 4
}

# Apply the ordinal encoding to the formatted_experience_level column
data_encoded['formatted_experience_level_encoded'] = data_encoded['formatted_experience_level'].map(experience_level_mapping)

# Handle missing values if any (e.g., fill with a default value or the most common level)
data_encoded['formatted_experience_level_encoded'].fillna(-1, inplace=True)

# Check the transformed data
data_encoded[['formatted_experience_level', 'formatted_experience_level_encoded']].head()

data_encoded.drop('formatted_experience_level', axis=1, inplace=True)

data_encoded.head()

"""**Preprocessing session two **"""

# Convert 'False' and 'True' to 0 and 1 respectively for the specified columns
data_encoded['pay_period_HOURLY'] = data_encoded['pay_period_HOURLY'].astype(int)
data_encoded['pay_period_MONTHLY'] = data_encoded['pay_period_MONTHLY'].astype(int)
data_encoded['pay_period_YEARLY'] = data_encoded['pay_period_YEARLY'].astype(int)

# Display the head of the DataFrame to check the transformations
data_encoded.head()

# prompt: fill all the empty values of the views column and the applies column with 0

data_encoded['views'].fillna(0, inplace=True)
data_encoded['applies'].fillna(0, inplace=True)

data_encoded.head()

filename = "data_encoded.xlsx"
data_encoded.to_excel(filename, index=False)

from google.colab import files

# Replace 'data_encoded.xlsx' with the path to the file you want to download
filename = 'data_encoded.xlsx'

# Use the files.download function to prompt a download in the browser
files.download(filename)

"""**Plotting graphs**"""

import matplotlib.pyplot as plt
import seaborn as sns

# Set the style of seaborn
sns.set(style="whitegrid")

# Prepare the figure
plt.figure(figsize=(12, 6))

# Plotting the distribution of maximum, median, and minimum salaries
sns.histplot(data['max_salary'], color="skyblue", label="Max Salary", kde=True, stat="density", linewidth=0)
sns.histplot(data['med_salary'], color="red", label="Median Salary", kde=True, stat="density", linewidth=0, alpha=0.7)
sns.histplot(data['min_salary'], color="green", label="Min Salary", kde=True, stat="density", linewidth=0, alpha=0.5)

# Adding legend
plt.legend()

# Adding titles and labels
plt.title("Distribution of Salary Ranges")
plt.xlabel("Salary")
plt.ylabel("Density")

# Show plot
plt.show()

plt.figure(figsize=(10, 6))

# Creating scatter plot
sns.scatterplot(data=data, x='views', y='applies', alpha=0.6, edgecolor=None)

# Adding titles and labels
plt.title('Relationship Between Number of Views and Applies')
plt.xlabel('Number of Views')
plt.ylabel('Number of Applies')

# Show plot
plt.show()

# Calculating the proportion of jobs allowing remote work versus those that don't
remote_counts = data_encoded['remote_allowed'].value_counts(normalize=True) * 100

# Preparing data for the pie chart
labels = 'Remote Allowed', 'Remote not Allowed'
sizes = [remote_counts[1], remote_counts[0]]
colors = ['lightblue', 'lightgreen']
explode = (0.1, 0)  # explode 1st slice

# Plotting the pie chart
plt.figure(figsize=(7, 7))
plt.pie(sizes, explode=explode, labels=labels, colors=colors, autopct='%1.1f%%',
        shadow=True, startangle=140)

plt.title('Proportion of Jobs Allowing Remote Work')
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.

# Show plot
plt.show()

# Calculating the counts for each pay period
pay_period_counts = data_encoded[['pay_period_HOURLY', 'pay_period_MONTHLY', 'pay_period_YEARLY']].sum()

# Preparing data for the pie chart
labels = pay_period_counts.index
sizes = pay_period_counts.values
colors = ['gold', 'lightcoral', 'lightskyblue']
explode = (0.1, 0, 0)  # explode the 1st slice (Hourly)

# Plotting the pie chart
plt.figure(figsize=(7, 7))
plt.pie(sizes, explode=explode, labels=labels, colors=colors, autopct='%1.1f%%',
        shadow=True, startangle=90)

plt.title('Distribution of Pay Periods Across All Jobs')
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.

# Show plot
plt.show()

# Setting up the figure for multiple subplots
fig, axs = plt.subplots(2, 2, figsize=(14, 12))

# List of the encoded features to plot
encoded_features = ['formatted_work_type_encoded', 'currency_encoded', 'compensation_type_encoded', 'formatted_experience_level_encoded']
titles = ['Work Type Encoded', 'Currency Encoded', 'Compensation Type Encoded', 'Experience Level Encoded']

for ax, feature, title in zip(axs.flat, encoded_features, titles):
    # Calculate counts for each category within the feature
    count = data_encoded[feature].value_counts()
    sns.barplot(x=count.index, y=count.values, ax=ax, palette="viridis")
    ax.set_title(title)
    ax.set_xlabel('')
    ax.set_ylabel('Frequency')

# Adjust layout
plt.tight_layout()
plt.show()

import numpy as np

# Exclude non-numeric columns and calculate the correlation matrix
numeric_data_encoded = data_encoded.select_dtypes(include=[np.number])
correlation_matrix = numeric_data_encoded.corr()

# Plotting the heatmap of the correlation matrix
plt.figure(figsize=(12, 10))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=.5, fmt=".2f")
plt.title('Correlation Matrix of Numeric Features')
plt.show()

"""# *** Clustering Job Titles Using K-Means using our own method***"""

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# Vectorizing the job titles
vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)  # limiting to 1000 features for simplicity
X_titles = vectorizer.fit_transform(data['title'])

# K-Means clustering
kmeans_titles = KMeans(n_clusters=5, random_state=0)
kmeans_titles.fit(X_titles)

# Adding cluster labels to the data
data['Title Cluster'] = kmeans_titles.labels_

# PCA for dimensionality reduction
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_titles.toarray())

# Plotting the clusters
plt.figure(figsize=(10, 6))
scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=kmeans_titles.labels_, cmap='viridis', marker='o', edgecolor='k')
plt.title('2D Visualization of Job Titles Clusters')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.colorbar(scatter)
plt.grid(True)
plt.show()

"""# **Evaluating the first cluster**"""

from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score

# Silhouette Score
silhouette_avg = silhouette_score(X_titles, kmeans_titles.labels_)

# Calinski-Harabasz Index
calinski_harabasz = calinski_harabasz_score(X_titles.toarray(), kmeans_titles.labels_)

# Davies-Bouldin Index
davies_bouldin = davies_bouldin_score(X_titles.toarray(), kmeans_titles.labels_)

#the metrcis are printed this way
silhouette_avg, calinski_harabasz, davies_bouldin
#Not optimal results, need to adjust clusters :(

"""# **Following our bad results, let's do parameter tuning**"""

from sklearn.metrics import silhouette_score
import numpy as np

# We'll test cluster numbers from 2 to 10 (a reasonable range for this size of dataset)
range_n_clusters = list(range(2, 11))
silhouette_avg_scores = []

for n_clusters in range_n_clusters:
    clusterer = KMeans(n_clusters=n_clusters, random_state=10)
    cluster_labels = clusterer.fit_predict(X_titles)
    silhouette_avg = silhouette_score(X_titles, cluster_labels)
    silhouette_avg_scores.append(silhouette_avg)

# Elbow method: Sum of squared distances of samples to their closest cluster center
sum_of_squared_distances = []
for n_clusters in range_n_clusters:
    kmeans = KMeans(n_clusters=n_clusters, random_state=10)
    kmeans.fit(X_titles)
    sum_of_squared_distances.append(kmeans.inertia_)

# Plot the silhouette scores
plt.figure(figsize=(14, 7))
plt.subplot(1, 2, 1)
plt.plot(range_n_clusters, silhouette_avg_scores, 'bx-')
plt.xlabel('Number of Clusters')
plt.ylabel('Silhouette Score')
plt.title('Silhouette Score for Various Numbers of Clusters')

# Plot the elbow curve
plt.subplot(1, 2, 2)
plt.plot(range_n_clusters, sum_of_squared_distances, 'bx-')
plt.xlabel('Number of Clusters')
plt.ylabel('Sum of Squared Distances')
plt.title('Elbow Method for Optimal Number of Clusters')
plt.show()

# Return the results as well
range_n_clusters, silhouette_avg_scores, sum_of_squared_distances

"""# **Performing the operation again with 7 clusters**"""

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# Vectorizing the job titles
vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)  # limiting to 1000 features for simplicity
X_titles = vectorizer.fit_transform(data['title'])

# K-Means clustering
kmeans_titles = KMeans(n_clusters=7, random_state=0)
kmeans_titles.fit(X_titles)

# Adding cluster labels to the data
data['Title Cluster'] = kmeans_titles.labels_

# PCA for dimensionality reduction
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_titles.toarray())

# Plotting the clusters
plt.figure(figsize=(10, 6))
scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=kmeans_titles.labels_, cmap='viridis', marker='o', edgecolor='k')
plt.title('2D Visualization of Job Titles Clusters')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.colorbar(scatter)
plt.grid(True)
plt.show()

#Trying again to see if we get better metrics
from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score

# Silhouette Score
silhouette_avg = silhouette_score(X_titles, kmeans_titles.labels_)

# Calinski-Harabasz Index
calinski_harabasz = calinski_harabasz_score(X_titles.toarray(), kmeans_titles.labels_)

# Davies-Bouldin Index
davies_bouldin = davies_bouldin_score(X_titles.toarray(), kmeans_titles.labels_)

#the metrcis are printed this way
silhouette_avg, calinski_harabasz, davies_bouldin
#better results, still shit lol

"""# **Alternative abbas method **"""

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import PCA
from sklearn.mixture import GaussianMixture
import matplotlib.pyplot as plt

# Vectorizing the job titles
#vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)  # limiting to 1000 features for simplicity
#X_titles = vectorizer.fit_transform(data['title'])

# GMM clustering
gmm_titles = GaussianMixture(n_components=2, random_state=0)
gmm_titles.fit(X_titles.toarray())  # GMM needs an array

# Adding cluster labels to the data
data['Title Cluster'] = gmm_titles.predict(X_titles.toarray())

# PCA for dimensionality reduction
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_titles.toarray())

# Plotting the clusters
plt.figure(figsize=(10, 6))
scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=data['Title Cluster'], cmap='viridis', marker='o', edgecolor='k')
plt.title('2D Visualization of Job Titles Clusters using GMM')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.colorbar(scatter)
plt.grid(True)
plt.show()

from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score

# Silhouette Score
# This score ranges from -1 to 1, where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters.
silhouette_avg = silhouette_score(X_titles, data['Title Cluster'])

# Calinski-Harabasz Index
# A higher score relates to a model with better defined clusters.
calinski_harabasz = calinski_harabasz_score(X_titles.toarray(), data['Title Cluster'])

# Davies-Bouldin Index
# The lowest score is better, with a score of 0 being the lowest possible. This index signifies the average 'similarity' between clusters, where the similarity is a measure that compares the distance between clusters with the size of the clusters themselves.
davies_bouldin = davies_bouldin_score(X_titles.toarray(), data['Title Cluster'])

print(f"Silhouette Score: {silhouette_avg}")
print(f"Calinski-Harabasz Index: {calinski_harabasz}")
print(f"Davies-Bouldin Index: {davies_bouldin}")

"""# **A final try using description techniques **"""

import re
import nltk
import matplotlib.pyplot as plt
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import TruncatedSVD
from sklearn.cluster import DBSCAN

# Text cleaning function
def clean_text(text):
    text = re.sub("(\\d|\\W)+", " ", text)
    text = text.lower()
    return text

# Tokenization and lemmatization
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')

stop_words = set(stopwords.words("english"))
lemmatizer = WordNetLemmatizer()

def tokenize_and_lemmatize(text):
    tokens = nltk.word_tokenize(text)
    filtered_tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words and len(token) > 3]
    return filtered_tokens

# Applying cleaning to the 'title' feature
data['Cleaned Title'] = data['title'].apply(clean_text)

# Vectorizing with TF-IDF
tfidf_vectorizer = TfidfVectorizer(tokenizer=tokenize_and_lemmatize, max_features=1000, stop_words='english', ngram_range=(1,2))
X_titles = tfidf_vectorizer.fit_transform(data['Cleaned Title'])

# Dimensionality reduction with SVD
svd = TruncatedSVD(n_components=50)
X_titles_reduced = svd.fit_transform(X_titles)
print("Explained variance ratio:", svd.explained_variance_ratio_.sum())

# Clustering with DBSCAN
dbscan = DBSCAN(eps=0.5, min_samples=5)
title_clusters = dbscan.fit_predict(X_titles_reduced)
data['Title Cluster'] = title_clusters

# Scatter plot of two main SVD components for titles
plt.figure(figsize=(12, 8))
scatter = plt.scatter(X_titles_reduced[:, 0], X_titles_reduced[:, 1], c=title_clusters, cmap='viridis', marker='o', edgecolor='k')
plt.title('2D Visualization of Job Titles Clusters')
plt.xlabel('SVD Component 1')
plt.ylabel('SVD Component 2')
plt.colorbar(scatter)
plt.show()

from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score

# You can only compute these scores if there are at least 2 clusters and fewer than n_samples - 1 clusters.
if len(set(title_clusters)) > 1 and len(set(title_clusters)) < len(X_titles_reduced) - 1:
    silhouette = silhouette_score(X_titles_reduced, title_clusters)
    davies_bouldin = davies_bouldin_score(X_titles_reduced, title_clusters)
    calinski_harabasz = calinski_harabasz_score(X_titles_reduced, title_clusters)

    print(f"Silhouette Score: {silhouette}")
    print(f"Davies-Bouldin Index: {davies_bouldin}")
    print(f"Calinski-Harabasz Index: {calinski_harabasz}")
else:
    print("Not enough clusters to evaluate. Consider adjusting your clustering parameters.")

"""# **Trying using spectral clustering**"""

import re
import nltk
import matplotlib.pyplot as plt
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import TruncatedSVD
from sklearn.cluster import SpectralClustering  # Import SpectralClustering

# Text cleaning function
def clean_text(text):
    text = re.sub("(\\d|\\W)+", " ", text)
    text = text.lower()
    return text

# Tokenization and lemmatization
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')

stop_words = set(stopwords.words("english"))
lemmatizer = WordNetLemmatizer()

def tokenize_and_lemmatize(text):
    tokens = nltk.word_tokenize(text)
    filtered_tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words and len(token) > 3]
    return filtered_tokens

# Applying cleaning to the 'title' feature
data['Cleaned Title'] = data['title'].apply(clean_text)

# Vectorizing with TF-IDF
tfidf_vectorizer = TfidfVectorizer(tokenizer=tokenize_and_lemmatize, max_features=1000, stop_words='english', ngram_range=(1,2))
X_titles = tfidf_vectorizer.fit_transform(data['Cleaned Title'])

# Dimensionality reduction with SVD
svd = TruncatedSVD(n_components=50)
X_titles_reduced = svd.fit_transform(X_titles)
print("Explained variance ratio:", svd.explained_variance_ratio_.sum())

# Clustering with Spectral Clustering
spectral = SpectralClustering(n_clusters=7, affinity='nearest_neighbors', assign_labels='kmeans')
title_clusters = spectral.fit_predict(X_titles_reduced)
data['Title Cluster'] = title_clusters

# Scatter plot of two main SVD components for titles
plt.figure(figsize=(12, 8))
scatter = plt.scatter(X_titles_reduced[:, 0], X_titles_reduced[:, 1], c=title_clusters, cmap='viridis', marker='o', edgecolor='k')
plt.title('2D Visualization of Job Titles Clusters')
plt.xlabel('SVD Component 1')
plt.ylabel('SVD Component 2')
plt.colorbar(scatter)
plt.show()

from sklearn import metrics

# Assuming 'X_titles_reduced' is your data reduced by SVD and 'title_clusters' are the labels obtained from clustering
silhouette_avg = metrics.silhouette_score(X_titles_reduced, title_clusters)
calinski_harabasz_score = metrics.calinski_harabasz_score(X_titles_reduced, title_clusters)
davies_bouldin_score = metrics.davies_bouldin_score(X_titles_reduced, title_clusters)

print("Silhouette Score: ", silhouette_avg)
print("Calinski-Harabasz Index: ", calinski_harabasz_score)
print("Davies-Bouldin Index: ", davies_bouldin_score)

"""# **Now using autoencoder (Deep learning model)**"""

import tensorflow as tf
import numpy as np
from keras.models import Model
from keras.layers import Input, Dense
from keras.optimizers import Adam
from sklearn.preprocessing import MinMaxScaler
import re
import nltk
import matplotlib.pyplot as plt
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import TruncatedSVD

# Text cleaning function
def clean_text(text):
    text = re.sub("(\\d|\\W)+", " ", text)
    text = text.lower()
    return text

# Tokenization and lemmatization
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')

stop_words = set(stopwords.words("english"))
lemmatizer = WordNetLemmatizer()

def tokenize_and_lemmatize(text):
    tokens = nltk.word_tokenize(text)
    filtered_tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words and len(token) > 3]
    return filtered_tokens

# Applying cleaning to the 'title' feature
data['Cleaned Title'] = data['title'].apply(clean_text)

# Vectorizing with TF-IDF
tfidf_vectorizer = TfidfVectorizer(tokenizer=tokenize_and_lemmatize, max_features=1000, stop_words='english', ngram_range=(1,2))
X_titles = tfidf_vectorizer.fit_transform(data['Cleaned Title'])

def build_autoencoder(input_dim, encoding_dim):
    # Input layer
    input_layer = Input(shape=(input_dim,))
    # Encoder layer
    encoded = Dense(encoding_dim, activation='relu')(input_layer)
    # Decoder layer
    decoded = Dense(input_dim, activation='sigmoid')(encoded)

    # Autoencoder model
    autoencoder = Model(input_layer, decoded)
    encoder = Model(input_layer, encoded)

    # Compile the model
    autoencoder.compile(optimizer=Adam(), loss='binary_crossentropy')
    return autoencoder, encoder

# Example: input_dim is the number of features from TF-IDF, encoding_dim is the reduced dimension
autoencoder, encoder = build_autoencoder(input_dim=1000, encoding_dim=50)



# Normalize the data
scaler = MinMaxScaler()
X_titles_norm = scaler.fit_transform(X_titles.toarray())

# Train the autoencoder
autoencoder.fit(X_titles_norm, X_titles_norm, epochs=50, batch_size=256, shuffle=True, validation_split=0.1)

from sklearn.preprocessing import MinMaxScaler

# Normalize the data
scaler = MinMaxScaler()
X_titles_norm = scaler.fit_transform(X_titles.toarray())

# Train the autoencoder
autoencoder.fit(X_titles_norm, X_titles_norm, epochs=50, batch_size=256, shuffle=True, validation_split=0.1)

X_titles_encoded = encoder.predict(X_titles_norm)

from sklearn.cluster import DBSCAN

# Example parameters; you might need to adjust these based on your dataset
dbscan = DBSCAN(eps=0.5, min_samples=5)
title_clusters = dbscan.fit_predict(X_titles_encoded)

# Check unique clusters formed (including -1 for noise)
print("Unique clusters:", np.unique(title_clusters))

from sklearn import metrics

# Silhouette Score can only be computed if more than one cluster is identified, excluding noise
if len(np.unique(title_clusters)) > 1:
    silhouette_avg = metrics.silhouette_score(X_titles_encoded, title_clusters)
    print("Silhouette Score: ", silhouette_avg)
    # Davies-Bouldin Index
    davies_bouldin = metrics.davies_bouldin_score(X_titles_encoded, title_clusters)
    print("Davies-Bouldin Index: ", davies_bouldin)
    # Calinski-Harabasz Index
    calinski_harabasz = metrics.calinski_harabasz_score(X_titles_encoded, title_clusters)
    print("Calinski-Harabasz Index: ", calinski_harabasz)
else:
    print("Not enough clusters to compute Silhouette Score.")


plt.figure(figsize=(10, 8))
# Filter out noise for visualization
core_samples_mask = np.zeros_like(title_clusters, dtype=bool)
core_samples_mask[dbscan.core_sample_indices_] = True
unique_labels = set(title_clusters)

# Colors and plot
colors = [plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]
for k, col in zip(unique_labels, colors):
    if k == -1:
        # Black used for noise.
        col = [0, 0, 0, 1]

    class_member_mask = (title_clusters == k)

    xy = X_titles_encoded[class_member_mask & core_samples_mask]
    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col), markeredgecolor='k', markersize=14)

    xy = X_titles_encoded[class_member_mask & ~core_samples_mask]
    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col), markeredgecolor='k', markersize=6)

plt.title('Estimated number of clusters: %d' % len(unique_labels))
plt.show()

"""# **Adding a small bonus**"""

import numpy as np

def get_top_features_cluster(tfidf_array, prediction, n_feats):
    labels = np.unique(prediction)
    dfs = []
    for label in labels:
        id_temp = np.where(prediction==label)  # indices for each cluster
        x_means = np.mean(tfidf_array[id_temp], axis = 0)  # mean tf-idf value for each feature in the cluster
        sorted_means = np.argsort(x_means)[::-1][:n_feats]  # indices with top 20 tf-idf values
        features = vectorizer.get_feature_names_out()
        best_features = [(features[i], x_means[i]) for i in sorted_means]
        df = pd.DataFrame(best_features, columns = ['features', 'score'])
        dfs.append(df)
    return dfs

# Get top features for each cluster
dfs = get_top_features_cluster(X_titles.toarray(), kmeans_titles.labels_, 10)

# Display top features for each cluster
for i, df in enumerate(dfs):
    print(f"Cluster {i} top words:\n{df}\n")

"""# **Clustering description**"""

import re
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer
import nltk

def clean_text(text):
    # Remove special characters and digits
    text = re.sub("(\\d|\\W)+", " ", text)
    text = text.lower()
    return text


nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')

# Lemmatization and stopword removal
stop_words = set(stopwords.words("english"))
lemmatizer = WordNetLemmatizer()

def tokenize_and_lemmatize(text):
    tokens = nltk.word_tokenize(text)
    filtered_tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words and len(token) > 3]
    return filtered_tokens

# Apply cleaning
data['Cleaned Description'] = data['description'].apply(clean_text)

# Vectorizing with TF-IDF
tfidf_vectorizer = TfidfVectorizer(tokenizer=tokenize_and_lemmatize, max_features=1000, stop_words='english', ngram_range=(1,2))
X_descriptions = tfidf_vectorizer.fit_transform(data['Cleaned Description'])

from sklearn.decomposition import TruncatedSVD

svd = TruncatedSVD(n_components=50)  # 50 components are typical but this number might need tuning
X_reduced = svd.fit_transform(X_descriptions)

print("Explained variance ratio:", svd.explained_variance_ratio_.sum())

from sklearn.cluster import DBSCAN

# Clustering with DBSCAN
dbscan = DBSCAN(eps=0.5, min_samples=5)  # these parameters may need adjustment
clusters = dbscan.fit_predict(X_reduced)

# Add cluster info to the DataFrame
data['Description Cluster'] = clusters

# Function to get top features from clusters
def get_top_features_cluster(tfidf_array, prediction, n_feats):
    labels = np.unique(prediction)
    dfs = []
    for label in labels:
        id_temp = np.where(prediction == label)  # indices for each cluster
        x_means = np.mean(tfidf_array[id_temp], axis=0)  # mean tf-idf value for each feature in the cluster
        sorted_means = np.argsort(x_means)[::-1][:n_feats]  # indices with top n tf-idf values
        features = tfidf_vectorizer.get_feature_names_out()
        best_features = [(features[i], x_means[i]) for i in sorted_means]
        df = pd.DataFrame(best_features, columns=['features', 'score'])
        dfs.append(df)
    return dfs

# Get top features for each cluster
description_dfs = get_top_features_cluster(X_descriptions.toarray(), clusters, 10)

# Display top features for each cluster
for i, df in enumerate(description_dfs):
    print(f"Cluster {i} top words:\n{df}\n")

from sklearn.decomposition import TruncatedSVD

svd = TruncatedSVD(n_components=50)  # 50 components are typical but this number might need tuning
X_reduced = svd.fit_transform(X_descriptions)

print("Explained variance ratio:", svd.explained_variance_ratio_.sum())

from sklearn.cluster import DBSCAN

# Clustering with DBSCAN
dbscan = DBSCAN(eps=0.5, min_samples=5)  # these parameters may need adjustment
clusters = dbscan.fit_predict(X_reduced)

# Add cluster info to the DataFrame
data['Description Cluster'] = clusters

# Function to get top features from clusters
def get_top_features_cluster(tfidf_array, prediction, n_feats):
    labels = np.unique(prediction)
    dfs = []
    for label in labels:
        id_temp = np.where(prediction == label)  # indices for each cluster
        x_means = np.mean(tfidf_array[id_temp], axis=0)  # mean tf-idf value for each feature in the cluster
        sorted_means = np.argsort(x_means)[::-1][:n_feats]  # indices with top n tf-idf values
        features = tfidf_vectorizer.get_feature_names_out()
        best_features = [(features[i], x_means[i]) for i in sorted_means]
        df = pd.DataFrame(best_features, columns=['features', 'score'])
        dfs.append(df)
    return dfs

# Get top features for each cluster
description_dfs = get_top_features_cluster(X_descriptions.toarray(), clusters, 10)

# Display top features for each cluster
for i, df in enumerate(description_dfs):
    print(f"Cluster {i} top words:\n{df}\n")

# Scatter plot of two main SVD components
plt.figure(figsize=(12, 8))
scatter = plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=clusters, cmap='viridis', marker='o', edgecolor='k')
plt.title('2D Visualization of Job Descriptions Clusters')
plt.xlabel('SVD Component 1')
plt.ylabel('SVD Component 2')
plt.colorbar(scatter)
plt.show()

from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score

# Filter out noise (-1 labels) for metric calculations
valid_clusters = X_reduced[clusters != -1]
valid_labels = clusters[clusters != -1]

if len(np.unique(valid_labels)) > 1:
    silhouette = silhouette_score(valid_clusters, valid_labels)
    calinski_harabasz = calinski_harabasz_score(valid_clusters, valid_labels)
    davies_bouldin = davies_bouldin_score(valid_clusters, valid_labels)
    print(f"Silhouette Score: {silhouette}")
    print(f"Calinski-Harabasz Index: {calinski_harabasz}")
    print(f"Davies-Bouldin Index: {davies_bouldin}")
else:
    print("Not enough clusters to calculate metrics (or all data is considered as noise).")

from nltk.sentiment import SentimentIntensityAnalyzer
import pandas as pd
import re
import numpy as np
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.sentiment import SentimentIntensityAnalyzer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import TruncatedSVD
from sklearn.cluster import DBSCAN
import matplotlib.pyplot as plt
nltk.download('vader_lexicon')

# Sentiment Analysis
sia = SentimentIntensityAnalyzer()
data['Sentiments'] = data['description'].apply(lambda x: sia.polarity_scores(x))
data['Compound'] = data['Sentiments'].apply(lambda x: x['compound'])

# Scatter plot of two main SVD components colored by sentiment scores
plt.figure(figsize=(12, 8))
scatter = plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=data['Compound'], cmap='coolwarm', marker='o', edgecolor='k')
plt.title('2D Visualization of Job Descriptions Clusters by Sentiment')
plt.xlabel('SVD Component 1')
plt.ylabel('SVD Component 2')
plt.colorbar(scatter, label='Sentiment Compound Score')
plt.show()

import joblib

# Save the trained model
joblib.dump(dbscan, 'dbscan_model.joblib')

"""# **Now on salaries**"""

scaler = StandardScaler()
data_encoded[['max_salary', 'med_salary', 'min_salary']] = scaler.fit_transform(data[['max_salary', 'med_salary', 'min_salary']])

# Select only the salary columns for clustering
scaled_salaries = data_encoded[['max_salary', 'med_salary', 'min_salary']]

# Initialize and fit the KMeans model on just the selected salary columns
kmeans = KMeans(n_clusters=5, random_state=42)
kmeans.fit(scaled_salaries)

# Store the cluster labels in the original data frame
data['Salary Cluster'] = kmeans.labels_

cluster_centers = kmeans.cluster_centers_
print("Cluster Centers:\n", cluster_centers)

# Map cluster centers to actual salary values if scaled (using min-max scaler as an example)
actual_cluster_centers = scaler.inverse_transform(cluster_centers)
print("Actual Salary Ranges in Clusters:\n", actual_cluster_centers)

# Descriptive statistics by cluster
for i in range(kmeans.n_clusters):
    cluster_data = data[data['Salary Cluster'] == i]
    print(f"Cluster {i} Statistics:\n", cluster_data[['max_salary', 'med_salary', 'min_salary']].describe())

from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

pca = PCA(n_components=2)
reduced_features = pca.fit_transform(scaled_salaries)

plt.figure(figsize=(10, 6))
plt.scatter(reduced_features[:, 0], reduced_features[:, 1], c=kmeans.labels_, cmap='viridis')
plt.title('2D PCA of Salary Clusters')
plt.xlabel('PCA Component 1')
plt.ylabel('PCA Component 2')
plt.colorbar()
plt.show()

from mpl_toolkits.mplot3d import Axes3D

fig = plt.figure(figsize=(10, 8))
ax = fig.add_subplot(111, projection='3d')
scatter = ax.scatter(data['max_salary'], data['med_salary'], data['min_salary'], c=data['Salary Cluster'], cmap='viridis')
ax.set_title('3D Scatter Plot of Salaries')
ax.set_xlabel('Max Salary')
ax.set_ylabel('Med Salary')
ax.set_zlabel('Min Salary')
plt.colorbar(scatter)
plt.show()

from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score

# Silhouette Score
silhouette_avg = silhouette_score(scaled_salaries, kmeans.labels_)

# Calinski-Harabasz Index
calinski_harabasz = calinski_harabasz_score(scaled_salaries, kmeans.labels_)

# Davies-Bouldin Index
davies_bouldin = davies_bouldin_score(scaled_salaries, kmeans.labels_)

print(f"Silhouette Score: {silhouette_avg}")
print(f"Calinski-Harabasz Index: {calinski_harabasz}")
print(f"Davies-Bouldin Index: {davies_bouldin}")

"""**Hadnling the rest of the features**"""

import pandas as pd
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import MinMaxScaler, OneHotEncoder
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
from sklearn.metrics import silhouette_score


# Impute missing numeric values with the mode
numeric_columns = data_encoded.select_dtypes(include=['int64', 'float64']).columns
mode_imputer = SimpleImputer(strategy='most_frequent')
data_encoded[numeric_columns] = mode_imputer.fit_transform(data_encoded[numeric_columns])

# Normalize 'views' and 'applies'
scaler = MinMaxScaler()
data_encoded[['views', 'applies']] = scaler.fit_transform(data_encoded[['views', 'applies']])

# Encode categorical data
categorical_columns = data_encoded.select_dtypes(include=['object']).columns
encoder = OneHotEncoder(sparse=False)
encoded_categories = encoder.fit_transform(data_encoded[categorical_columns])
encoded_cat_df = pd.DataFrame(encoded_categories, columns=encoder.get_feature_names_out(categorical_columns))
data_encoded = pd.concat([data_encoded.drop(categorical_columns, axis=1), encoded_cat_df], axis=1)

# Apply PCA for dimensionality reduction
pca = PCA(n_components=2)  # Reduce to 2 dimensions for visualization
data_pca = pca.fit_transform(data_encoded)

# Determine the optimal number of clusters using the Elbow Method
sse = []
for k in range(1, 11):
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(data_pca)
    sse.append(kmeans.inertia_)

# Plot SSE values for each k
plt.figure(figsize=(10, 6))
plt.plot(range(1, 11), sse, marker='o')
plt.title('Elbow Method')
plt.xlabel('Number of clusters')
plt.ylabel('SSE')
plt.grid(True)
plt.show()

# Choose the optimal number of clusters (let's say 3 based on the plot) and apply K-Means
k_optimal = 3
kmeans = KMeans(n_clusters=k_optimal, random_state=42)
clusters = kmeans.fit_predict(data_pca)

# Evaluate clustering using silhouette score
sil_score = silhouette_score(data_pca, clusters)
print(f'Silhouette Score for k={k_optimal}: {sil_score}')

# Visualize the clusters
plt.figure(figsize=(8, 6))
plt.scatter(data_pca[:, 0], data_pca[:, 1], c=clusters, cmap='viridis', marker='o', edgecolor='black')
plt.title('Visualization of clustered data')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.colorbar()
plt.grid(True)
plt.show()

# Print Silhouette Score
print(f"Silhouette Score for k={k_optimal}: {sil_score}")

# Print the SSE values for each k
for k, s in zip(range(1, 11), sse):
    print(f"SSE for k={k}: {s}")

from sklearn.metrics import davies_bouldin_score, calinski_harabasz_score

db_index = davies_bouldin_score(data_pca, clusters)
ch_index = calinski_harabasz_score(data_pca, clusters)

print(f"Davies-Bouldin Index: {db_index}")
print(f"Calinski-Harabasz Index: {ch_index}")